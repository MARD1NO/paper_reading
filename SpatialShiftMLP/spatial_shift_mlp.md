论文：https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2106.07477

### 主要思想

MLP-Mixer中，为了引入patch之间的交互，他做了个token_mixing，简单来说就是转置+全连接层



作者认为其实这等价于一个有全局感受野+空间特异性的Depthwise卷积，

容易有过拟合风险（这也是为什么需要大数据集上训练效果才好）



作者从TSM中移位操作得到启发，抛弃了token-mixing这套，而是对特征图在通道维度上进行分组，然后进行上下左右，四种移位操作，然后送入全连接层等。



本质上还是加入了归纳偏置，才能在小数据表现的不错。

